# -*- coding: utf-8 -*-
import os
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay,
    accuracy_score, classification_report
)

# â€”â€”â€” ä¸­æ–‡å­—é«”è¨­å®šï¼ˆæ²’æœ‰é€™å­—é«”ä¹Ÿä¸å½±éŸ¿è¨“ç·´ï¼Œåªå½±éŸ¿åœ–è¡¨é¡¯ç¤ºï¼‰â€”â€”â€”
plt.rcParams["font.family"]        = ["Microsoft JhengHei"]
plt.rcParams["axes.unicode_minus"] = False


def _read_one(path: Path) -> pd.DataFrame:
    """è®€ä¸€å€‹ CSV æˆ– Excelï¼ˆå¸¶ç·¨ç¢¼å®¹éŒ¯ï¼›Excel æœƒæŠŠæ‰€æœ‰å·¥ä½œè¡¨åˆä½µï¼‰"""
    path = Path(path)
    suf = path.suffix.lower()
    if suf == ".csv":
        try:
            return pd.read_csv(path, encoding="utf-8-sig")
        except UnicodeDecodeError:
            return pd.read_csv(path, encoding="utf-8")
    elif suf in (".xls", ".xlsx"):
        x = pd.read_excel(path, sheet_name=None, engine="openpyxl")  # è®€å…¨éƒ¨å·¥ä½œè¡¨
        if isinstance(x, dict):
            df = pd.concat(x.values(), ignore_index=True)
        else:
            df = x
        return df
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼š{path}")


def _load_and_concat(paths_or_dirs) -> pd.DataFrame:
    """æ”¯æ´å¤šæª”/è³‡æ–™å¤¾è¼¸å…¥ï¼Œè‡ªå‹•åˆä½µæˆä¸€å€‹ DataFrame"""
    files = []
    for p in paths_or_dirs if isinstance(paths_or_dirs, (list, tuple)) else [paths_or_dirs]:
        p = Path(p)
        if p.is_dir():
            files += sorted(list(p.rglob("*.csv")) + list(p.rglob("*.xlsx")) + list(p.rglob("*.xls")))
        else:
            files.append(p)
    if not files:
        raise FileNotFoundError("æ‰¾ä¸åˆ°ä»»ä½• .csv/.xlsx æª”æ¡ˆ")

    dfs = []
    for f in files:
        df = _read_one(f)
        df.columns = df.columns.str.strip()
        dfs.append(df)
        print(f"âœ” è®€å–ï¼š{f}  shape={df.shape}")
    out = pd.concat(dfs, ignore_index=True, sort=False)
    out.columns = out.columns.str.strip()
    return out


def train_and_save_confusion(dataset_paths, target_column: str = "label"):
    """
    dataset_paths: str æˆ– List[str]ï¼›å¯åŒæ™‚ä¸Ÿå¤šå€‹ç‰¹å¾µæª”/è³‡æ–™å¤¾
    target_column: ç›®æ¨™æ¬„ä½åï¼ˆé è¨­ 'label'ï¼‰
    """
    # 1) è®€å–ä¸¦åˆä½µ
    df = _load_and_concat(dataset_paths)

    # ç›®æ¨™æ¬„ä½å®¹éŒ¯ï¼ˆå¤§å°å¯« / ç©ºç™½ï¼‰
    colmap = {c.lower(): c for c in df.columns}
    if target_column.lower() not in colmap:
        raise ValueError(f"âŒ æ‰¾ä¸åˆ°æ¨™ç±¤æ¬„ä½ï¼š{target_column}ï¼ˆç›®å‰æ¬„ä½ï¼š{list(df.columns)}ï¼‰")
    target_column = colmap[target_column.lower()]

    # æ¸…ç†æ¨™ç±¤å…§å®¹
    df = df.dropna(subset=[target_column]).copy()
    df[target_column] = df[target_column].astype(str).str.strip()

    # 2) ç‰¹å¾µ/æ¨™ç±¤åˆ†é›¢
    X = df.drop(columns=[target_column])
    y = df[target_column].astype(str)

    # 3) One-Hotï¼ˆå°‡é¡åˆ¥æ¬„ä½å¦‚ axis è½‰ç‚ºæ•¸å€¼ï¼‰
    X = pd.get_dummies(X, drop_first=False)

    # 4) åˆ†å‰²è³‡æ–™
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    print("è³‡æ–™é‡ï¼š", X.shape, "\né¡åˆ¥åˆ†å¸ƒï¼š")
    print(y.value_counts())

    # äº¤å‰é©—è­‰æŠ˜æ•¸å–æ±ºæ–¼æœ€å°é¡åˆ¥æ¨£æœ¬æ•¸
    min_class_count = y_train.value_counts().min()
    cv_folds = min(5, int(min_class_count))
    if cv_folds < 2:
        raise ValueError(f"âŒ ç„¡æ³•äº¤å‰é©—è­‰ï¼šæŸé¡åˆ¥æ¨£æœ¬æ•¸åƒ… {min_class_count}ï¼Œè‡³å°‘éœ€ 2ã€‚")
    print(f"â†’ ä½¿ç”¨ cv = {cv_folds} é€²è¡Œäº¤å‰é©—è­‰")

    # 5) GridSearchCV æ‰¾æœ€ä½³åƒæ•¸
    rf = RandomForestClassifier(random_state=42, n_jobs=-1)
    param_grid = {
        'n_estimators':      [100, 200, 300],
        'max_depth':         [None, 10, 20],
        'min_samples_split': [2, 5]
    }
    grid = GridSearchCV(rf, param_grid, cv=cv_folds, scoring='accuracy', n_jobs=-1)
    print("â†’ æ­£åœ¨é€²è¡Œè¶…åƒæ•¸èª¿æ ¡...")
    grid.fit(X_train, y_train)
    best_rf = grid.best_estimator_
    print(f"âœ… æœ€ä½³åƒæ•¸ï¼š{grid.best_params_}")

    # 6) è©•ä¼°
    y_pred = best_rf.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"â†’ æ¸¬è©¦é›†æº–ç¢ºç‡ï¼š{acc:.4f}")
    print("â†’ åˆ†é¡å ±å‘Šï¼š")
    print(classification_report(y_test, y_pred, digits=4))

    # 7) è¼¸å‡ºè³‡æ–™å¤¾ï¼ˆåˆä½µæƒ…å¢ƒçµ±ä¸€æ”¾ combinedï¼‰
    output_root = Path.cwd() / "0809output_rf" / "combined"
    images_dir = output_root / "images"
    models_dir = output_root / "models"
    images_dir.mkdir(parents=True, exist_ok=True)
    models_dir.mkdir(parents=True, exist_ok=True)

    # 8) æ··æ·†çŸ©é™£ PNG
    cm = confusion_matrix(y_test, y_pred, labels=best_rf.classes_)
    disp = ConfusionMatrixDisplay(cm, display_labels=best_rf.classes_)
    fig, ax = plt.subplots(figsize=(8, 6))
    disp.plot(ax=ax, cmap='Blues', colorbar=True, values_format='d')
    plt.title('Confusion Matrix')
    png_path = images_dir / "combined_æ··æ·†çŸ©é™£.png"
    plt.savefig(png_path, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"ğŸ“Š æ··æ·†çŸ©é™£å·²å„²å­˜ï¼š{png_path}")

    # 9) ç‰¹å¾µé‡è¦æ€§ï¼ˆåˆ—å‰ 20 åï¼‰
    importances = best_rf.feature_importances_
    features = X.columns
    ranked = sorted(zip(features, importances), key=lambda x: x[1], reverse=True)
    print("\nâ†’ ç‰¹å¾µé‡è¦æ€§ï¼ˆå‰ 20ï¼‰ï¼š")
    for name, score in ranked[:20]:
        print(f"{name}: {score:.4f}")

    # 10) æ¨¡å‹ + æ¬„ä½é †åº ä¸€èµ·å„²å­˜
    artifact = {
        "model": best_rf,
        "feature_columns": list(X.columns)
    }
    model_path = models_dir / "combined_model.pkl"
    joblib.dump(artifact, model_path)
    print(f"ğŸ§  æ¨¡å‹å·²å„²å­˜ï¼š{model_path}")


# â€”â€”â€” é€™è£¡ã€Œå¯«æ­»ã€ä½ çš„æª”æ¡ˆè·¯å¾‘èˆ‡æ¨™ç±¤æ¬„ä½ â€”â€”â€”
if __name__ == "__main__":
    DATA_FILE = "C:\\Users\\User\\py\\Bicycle_Center_E-Bikes\\0809Training_set_merging\\0809å¹³è·¯_é¡›ç°¸.xlsx"
    TARGET    = "label"   # å¦‚æœä½ çš„æ¨™ç±¤æ¬„ä½ä¸æ˜¯ labelï¼Œæ”¹é€™è£¡

    train_and_save_confusion(DATA_FILE, TARGET)
