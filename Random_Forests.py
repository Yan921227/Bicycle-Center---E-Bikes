# -*- coding: utf-8 -*-
import os
from pathlib import Path
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib
from datetime import datetime
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay,
    accuracy_score, classification_report
)

# â€”â€”â€” ä¸­æ–‡å­—é«”è¨­å®šï¼ˆæ²’æœ‰é€™å­—é«”ä¹Ÿä¸å½±éŸ¿è¨“ç·´ï¼Œåªå½±éŸ¿åœ–è¡¨é¡¯ç¤ºï¼‰â€”â€”â€”
plt.rcParams["font.family"]        = ["Microsoft JhengHei"]
plt.rcParams["axes.unicode_minus"] = False


def _read_one(path: Path) -> pd.DataFrame:
    """è®€ä¸€å€‹ CSV æˆ– Excelï¼ˆå¸¶ç·¨ç¢¼å®¹éŒ¯ï¼›Excel æœƒæŠŠæ‰€æœ‰å·¥ä½œè¡¨åˆä½µï¼‰"""
    path = Path(path)
    suf = path.suffix.lower()
    if suf == ".csv":
        try:
            return pd.read_csv(path, encoding="utf-8-sig")
        except UnicodeDecodeError:
            return pd.read_csv(path, encoding="utf-8")
    elif suf in (".xls", ".xlsx"):
        x = pd.read_excel(path, sheet_name=None, engine="openpyxl")  # è®€å…¨éƒ¨å·¥ä½œè¡¨
        if isinstance(x, dict):
            df = pd.concat(x.values(), ignore_index=True)
        else:
            df = x
        return df
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼š{path}")


def _load_and_concat(paths_or_dirs) -> pd.DataFrame:
    """æ”¯æ´å¤šæª”/è³‡æ–™å¤¾è¼¸å…¥ï¼Œè‡ªå‹•åˆä½µæˆä¸€å€‹ DataFrame"""
    files = []
    for p in paths_or_dirs if isinstance(paths_or_dirs, (list, tuple)) else [paths_or_dirs]:
        p = Path(p)
        if p.is_dir():
            files += sorted(list(p.rglob("*.csv")) + list(p.rglob("*.xlsx")) + list(p.rglob("*.xls")))
        else:
            files.append(p)
    if not files:
        raise FileNotFoundError("æ‰¾ä¸åˆ°ä»»ä½• .csv/.xlsx æª”æ¡ˆ")

    dfs = []
    for f in files:
        df = _read_one(f)
        df.columns = df.columns.str.strip()
        dfs.append(df)
        print(f"âœ” è®€å–ï¼š{f}  shape={df.shape}")
    out = pd.concat(dfs, ignore_index=True, sort=False)
    out.columns = out.columns.str.strip()
    return out


# ========== é€™æ®µæ˜¯ã€Œé¡å¤–è¼¸å‡º STM32 ç”¨ .hã€çš„é—œéµå‡½å¼ ==========
def export_rf_to_c_header(rf, feature_names, class_names, out_path: Path, model_id: str = "RF_MODEL"):
    """
    å°‡ sklearn RandomForestClassifier åŒ¯å‡ºç‚º STM32 å¯ç”¨çš„ .hã€‚
    - rf            : å·²è¨“ç·´å¥½çš„ RandomForestClassifier
    - feature_names : è¨“ç·´å¾Œï¼ˆOne-Hot å¾Œï¼‰çš„æ¬„ä½é †åº
    - class_names   : é¡åˆ¥åç¨±ï¼ˆå­—ä¸²åˆ—è¡¨ï¼‰ï¼Œè‹¥æœªå°é½Šæœƒè‡ªå‹•æ”¹ç”¨ rf.classes_
    - out_path      : ç›®æ¨™ .h è·¯å¾‘
    - model_id      : C ç¬¦è™Ÿå‰ç¶´ï¼ˆå°‡è‡ªå‹•éæ¿¾ç‚º A-Z0-9_ï¼Œä¸¦è½‰å¤§å¯«ï¼‰
    ç”¢å‡ºå…§å®¹ï¼š
      - <model>.h              ï¼šå¯ç›´æ¥åœ¨ MCU ç«¯ #include ä¸¦å‘¼å« <MODEL>_predict(x)
      - <model>.features.txt   ï¼šç‰¹å¾µé †åºå°ç…§ï¼ŒMCU ç«¯çµ„ x[] è¦å®Œå…¨ä¸€è‡´
      - <model>.classes.txt    ï¼šé¡åˆ¥åç¨±å°ç…§
    """
    out_path = Path(out_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    if not hasattr(rf, "estimators_"):
        raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´ï¼ˆç¼ºå°‘ estimators_ï¼‰ã€‚")
    if len(getattr(rf, "classes_", [])) != len(class_names):
        class_names = [str(c) for c in rf.classes_]

    # ä¹¾æ·¨çš„ C ç¬¦è™Ÿå‰ç¶´
    model_id = re.sub(r"\W+", "_", str(model_id)).upper()
    if not model_id or model_id[0].isdigit():
        model_id = "RF_MODEL"

    n_trees     = len(rf.estimators_)
    n_features  = len(feature_names)
    n_classes   = len(class_names)
    node_counts = [est.tree_.node_count for est in rf.estimators_]
    max_nodes   = max(node_counts) if node_counts else 0

    # è‹¥ç¯€é»æˆ–ç‰¹å¾µéå¤šï¼Œè‡ªå‹•åˆ‡æ› int32ï¼Œä»¥å… int16 æº¢ä½
    use_int32 = (n_features > 32767) or (max_nodes > 32767)

    def f32(v: float) -> str:
        if v is None or not np.isfinite(v):
            return "0.0f"
        s = f"{float(v):.9g}"
    # å¦‚æœå­—ä¸²ä¸­æ²’æœ‰å°æ•¸é»ï¼Œå°±è£œä¸Š .0fï¼›å¦å‰‡è£œ f
        if '.' not in s and 'e' not in s and 'E' not in s:
            return s + ".0f"
        return s + "f"



    def c_str(s: str) -> str:
        return '"' + str(s).replace("\\", "\\\\").replace('"', '\\"') + '"'

    lines = []
    lines.append("// -----------------------------------------------------------------------------")
    lines.append("//  Auto-generated RandomForest header for STM32")
    lines.append(f"//  Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("//  NOTE: è«‹ç”¨èˆ‡è¨“ç·´æ™‚ One-Hot å¾Œå®Œå…¨ç›¸åŒçš„ç‰¹å¾µé †åºèˆ‡é•·åº¦ã€‚")
    lines.append("// -----------------------------------------------------------------------------\n")

    guard = f"{model_id}_H"
    lines.append(f"#ifndef {guard}")
    lines.append(f"#define {guard}\n")
    lines.append("#include <stdint.h>\n")
    lines.append("#ifdef __cplusplus")
    lines.append('extern "C" {')
    lines.append("#endif\n")

    lines.append(f"#define {model_id}_N_TREES    {n_trees}")
    lines.append(f"#define {model_id}_N_FEATURES {n_features}")
    lines.append(f"#define {model_id}_N_CLASSES  {n_classes}")
    if use_int32:
        lines.append(f"#define {model_id}_USE_INT32 1")
    lines.append(f"#define {model_id}_LEAF_INDEX (-1)\n")

    idx_t = "int32_t" if use_int32 else "int16_t"
    lines.append(f"typedef {idx_t} {model_id}_idx_t;")
    lines.append("typedef struct {")
    lines.append(f"  {model_id}_idx_t feature;   // å…§éƒ¨ç‰¹å¾µç´¢å¼•ï¼ˆéè‘‰ï¼‰")
    lines.append( "  float          threshold;  // åˆ†è£‚é–€æª»ï¼ˆéè‘‰ï¼‰")
    lines.append(f"  {model_id}_idx_t left;      // å·¦å­ï¼›è‘‰ç¯€é»ç‚º -1")
    lines.append(f"  {model_id}_idx_t right;     // å³å­ï¼›è‘‰ç¯€é»ç‚º -1")
    lines.append(f"  {model_id}_idx_t value;     // è‘‰ç¯€é»çš„é¡åˆ¥ç´¢å¼•")
    lines.append(f"}} {model_id}_Node;\n")

    # è‹¥éœ€è¦æŠŠåç¨±ä¹Ÿç·¨é€² MCUï¼ˆçœç•¥å¯çœç©ºé–“ï¼‰ï¼šç·¨è­¯åŠ  -D{model_id}_EMBED_STRINGS
    lines.append(f"#ifdef {model_id}_EMBED_STRINGS")
    feat_str = ", ".join(c_str(n) for n in feature_names)
    cls_str  = ", ".join(c_str(c) for c in class_names)
    lines.append(f"static const char* {model_id}_FEATURE_NAMES[{n_features}] = {{ {feat_str} }};")
    lines.append(f"static const char* {model_id}_CLASS_NAMES[{n_classes}]   = {{ {cls_str}  }};")
    lines.append("#endif\n")

    tree_size_list = []
    for ti, est in enumerate(rf.estimators_):
        t = est.tree_
        left      = t.children_left.tolist()
        right     = t.children_right.tolist()
        feature   = t.feature.tolist()
        threshold = t.threshold.tolist()
        values    = t.value.squeeze(axis=1) if t.value.ndim == 3 else t.value
        if values.ndim == 1:
            values = np.expand_dims(values, axis=1)

        n_nodes = t.node_count
        tree_size_list.append(n_nodes)

        lines.append(f"static const {model_id}_Node {model_id}_TREE_{ti}[] = {{")
        rows = []
        for i in range(n_nodes):
            is_leaf = (left[i] == -1 and right[i] == -1)
            pred_cls = int(np.argmax(values[i])) if is_leaf else 0
            thr = 0.0 if is_leaf else float(threshold[i])
            fi  = -1 if is_leaf else int(feature[i])
            rows.append(
                f"  {{ ({model_id}_idx_t){fi}, {f32(thr)}, "
                f"({model_id}_idx_t){left[i]}, ({model_id}_idx_t){right[i]}, "
                f"({model_id}_idx_t){pred_cls} }}"
            )
        lines.append(",\n".join(rows))
        lines.append("};\n")

    ptrs  = ", ".join([f"{model_id}_TREE_{ti}" for ti in range(n_trees)])
    sizes = ", ".join([str(s) for s in tree_size_list])
    lines.append(f"static const {model_id}_Node* const {model_id}_FOREST[{n_trees}] = {{ {ptrs} }};")
    lines.append(f"static const {model_id}_idx_t {model_id}_TREE_SIZES[{n_trees}] = {{ {sizes} }};\n")

    # å…§è¯æ¨è«–å‡½å¼ï¼ˆæŠ•ç¥¨ï¼‰
    lines.append(f"static inline int {model_id}_predict(const float* x) {{")
    lines.append( f"  int votes[{model_id}_N_CLASSES] = {{0}};")
    lines.append( f"  for (int t = 0; t < {model_id}_N_TREES; ++t) {{")
    lines.append( f"    const {model_id}_Node* nodes = {model_id}_FOREST[t];")
    lines.append(  f"    {model_id}_idx_t idx = 0;")
    lines.append(  "    while (1) {")
    lines.append(  f"      const {model_id}_Node* n = &nodes[idx];")
    lines.append(  f"      if (n->left == {model_id}_LEAF_INDEX && n->right == {model_id}_LEAF_INDEX) {{ votes[n->value]++; break; }}")
    lines.append(  "      const float v = x[n->feature];")
    lines.append(  "      idx = (v <= n->threshold) ? n->left : n->right;")
    lines.append(  "    }")
    lines.append(  "  }")
    lines.append(  "  int best = 0; int bestv = votes[0];")
    lines.append(  f"  for (int c = 1; c < {model_id}_N_CLASSES; ++c) {{ if (votes[c] > bestv) {{ bestv = votes[c]; best = c; }} }}")
    lines.append(  "  return best;")
    lines.append(  "}\n")

    # ï¼ˆé¸é…ï¼‰è¼¸å‡ºæŠ•ç¥¨æ¯”ä¾‹ï¼ˆç°¡æ˜“æ©Ÿç‡ï¼‰
    lines.append(f"static inline void {model_id}_predict_proba(const float* x, float out[{model_id}_N_CLASSES]) {{")
    lines.append( f"  int votes[{model_id}_N_CLASSES] = {{0}};")
    lines.append( f"  for (int t = 0; t < {model_id}_N_TREES; ++t) {{")
    lines.append( f"    const {model_id}_Node* nodes = {model_id}_FOREST[t];")
    lines.append(  f"    {model_id}_idx_t idx = 0;")
    lines.append(  "    while (1) {")
    lines.append(  f"      const {model_id}_Node* n = &nodes[idx];")
    lines.append(  f"      if (n->left == {model_id}_LEAF_INDEX && n->right == {model_id}_LEAF_INDEX) {{ votes[n->value]++; break; }}")
    lines.append(  "      const float v = x[n->feature];")
    lines.append(  "      idx = (v <= n->threshold) ? n->left : n->right;")
    lines.append(  "    }")
    lines.append(  "  }")
    lines.append( f"  for (int c = 0; c < {model_id}_N_CLASSES; ++c) out[c] = (float)votes[c] / (float){model_id}_N_TREES;")
    lines.append(  "}\n")

    lines.append("#ifdef __cplusplus")
    lines.append("} // extern \"C\"")
    lines.append("#endif")
    lines.append(f"#endif // {guard}\n")

    out_path.write_text("\n".join(lines), encoding="utf-8")
    print(f"ğŸ§¾ å·²è¼¸å‡º C headerï¼š{out_path}  ï¼ˆç¯€é»ç¸½æ•¸ï¼š{sum(node_counts)}ï¼Œæ¨¹æ•¸ï¼š{n_trees}ï¼‰")

    # åŒæ­¥è¼¸å‡ºå°ç…§æ¸…å–®ï¼ˆMCU ç«¯çµ„è£ç‰¹å¾µæ™‚è¦å°é½Šï¼‰
    out_path.with_suffix(".features.txt").write_text("\n".join(map(str, feature_names)), encoding="utf-8")
    out_path.with_suffix(".classes.txt").write_text("\n".join(map(str, class_names)), encoding="utf-8")
# ==========ï¼ˆé—œéµå‡½å¼åˆ°æ­¤ï¼‰==========


def train_and_save_confusion(dataset_paths, target_column: str = "label"):
    """
    dataset_paths: str æˆ– List[str]ï¼›å¯åŒæ™‚ä¸Ÿå¤šå€‹ç‰¹å¾µæª”/è³‡æ–™å¤¾
    target_column: ç›®æ¨™æ¬„ä½åï¼ˆé è¨­ 'label'ï¼‰
    """
    # 1) è®€å–ä¸¦åˆä½µ
    df = _load_and_concat(dataset_paths)

    # ç›®æ¨™æ¬„ä½å®¹éŒ¯ï¼ˆå¤§å°å¯« / ç©ºç™½ï¼‰
    colmap = {c.lower(): c for c in df.columns}
    if target_column.lower() not in colmap:
        raise ValueError(f"âŒ æ‰¾ä¸åˆ°æ¨™ç±¤æ¬„ä½ï¼š{target_column}ï¼ˆç›®å‰æ¬„ä½ï¼š{list(df.columns)}ï¼‰")
    target_column = colmap[target_column.lower()]

    # æ¸…ç†æ¨™ç±¤å…§å®¹
    df = df.dropna(subset=[target_column]).copy()
    df[target_column] = df[target_column].astype(str).str.strip()

    # 2) ç‰¹å¾µ/æ¨™ç±¤åˆ†é›¢
    X = df.drop(columns=[target_column])
    y = df[target_column].astype(str)

    # 3) One-Hotï¼ˆå°‡é¡åˆ¥æ¬„ä½å¦‚ axis è½‰ç‚ºæ•¸å€¼ï¼‰
    X = pd.get_dummies(X, drop_first=False)

    # 4) åˆ†å‰²è³‡æ–™
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    print("è³‡æ–™é‡ï¼š", X.shape, "\né¡åˆ¥åˆ†å¸ƒï¼š")
    print(y.value_counts())

    # äº¤å‰é©—è­‰æŠ˜æ•¸å–æ±ºæ–¼æœ€å°é¡åˆ¥æ¨£æœ¬æ•¸
    min_class_count = y_train.value_counts().min()
    cv_folds = min(5, int(min_class_count))
    if cv_folds < 2:
        raise ValueError(f"âŒ ç„¡æ³•äº¤å‰é©—è­‰ï¼šæŸé¡åˆ¥æ¨£æœ¬æ•¸åƒ… {min_class_count}ï¼Œè‡³å°‘éœ€ 2ã€‚")
    print(f"â†’ ä½¿ç”¨ cv = {cv_folds} é€²è¡Œäº¤å‰é©—è­‰")

    # 5) GridSearchCV æ‰¾æœ€ä½³åƒæ•¸
    rf = RandomForestClassifier(random_state=42, n_jobs=-1)
    param_grid = {
        'n_estimators':      [40],
        'max_depth':         [10],
        'min_samples_split': [2, 5]
    }
    grid = GridSearchCV(rf, param_grid, cv=cv_folds, scoring='accuracy', n_jobs=-1)
    print("â†’ æ­£åœ¨é€²è¡Œè¶…åƒæ•¸èª¿æ ¡...")
    grid.fit(X_train, y_train)
    best_rf = grid.best_estimator_
    print(f"âœ… æœ€ä½³åƒæ•¸ï¼š{grid.best_params_}")

    # 6) è©•ä¼°
    y_pred = best_rf.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"â†’ æ¸¬è©¦é›†æº–ç¢ºç‡ï¼š{acc:.4f}")
    print("â†’ åˆ†é¡å ±å‘Šï¼š")
    print(classification_report(y_test, y_pred, digits=4))

    # 7) è¼¸å‡ºè³‡æ–™å¤¾ï¼ˆåˆä½µæƒ…å¢ƒçµ±ä¸€æ”¾ combinedï¼‰
    output_root = Path.cwd() / "1020output_rf" / "combined"
    images_dir = output_root / "images"
    models_dir = output_root / "models"
    images_dir.mkdir(parents=True, exist_ok=True)
    models_dir.mkdir(parents=True, exist_ok=True)

    # 8) æ··æ·†çŸ©é™£ PNG
    cm = confusion_matrix(y_test, y_pred, labels=best_rf.classes_)
    disp = ConfusionMatrixDisplay(cm, display_labels=best_rf.classes_)
    fig, ax = plt.subplots(figsize=(8, 6))
    disp.plot(ax=ax, cmap='Blues', colorbar=True, values_format='d')
    plt.title('Confusion Matrix')
    png_path = images_dir / "å…¨æ•¸æ“š_æ··æ·†çŸ©é™£.png"
    plt.savefig(png_path, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"ğŸ“Š æ··æ·†çŸ©é™£å·²å„²å­˜ï¼š{png_path}")

    # 9) ç‰¹å¾µé‡è¦æ€§ï¼ˆåˆ—å‰ 20 åï¼‰
    importances = best_rf.feature_importances_
    features = X.columns
    ranked = sorted(zip(features, importances), key=lambda x: x[1], reverse=True)
    print("\nâ†’ ç‰¹å¾µé‡è¦æ€§ï¼ˆå‰ 20ï¼‰ï¼š")
    for name, score in ranked[:20]:
        print(f"{name}: {score:.4f}")

    # 10) æ¨¡å‹ + æ¬„ä½é †åº ä¸€èµ·å„²å­˜ï¼ˆ.pklï¼‰
    artifact = {
        "model": best_rf,   
        "feature_columns": list(X.columns),
        "class_names": list(best_rf.classes_)
    }
    model_path = models_dir / "rf_all_data_model.pkl"  # ä¿ç•™ .pkl ä¾› Python ä½¿ç”¨
    joblib.dump(artifact, model_path)
    print(f"ğŸ§  æ¨¡å‹å·²å„²å­˜ï¼š{model_path}")

    # 11) é¡å¤–è¼¸å‡º STM32 ç”¨çš„ .h
    header_path = models_dir / "rf_all_data.h"  # C Header
    export_rf_to_c_header(
        best_rf,
        feature_names=list(X.columns),
        class_names=list(best_rf.classes_),
        out_path=header_path,
        model_id="rf_all_data"  # C ç¬¦è™Ÿå‰ç¶´ï¼ˆå®èˆ‡å‡½å¼åæœƒç”¨åˆ°ï¼‰
    )
    print("âœ… å·²å®Œæˆï¼š.pklã€.hã€.features.txtã€.classes.txt å…¨éƒ¨è¼¸å‡ºå®Œæˆã€‚")


# â€”â€”â€” é€™è£¡ã€Œå¯«æ­»ã€ä½ çš„æª”æ¡ˆè·¯å¾‘èˆ‡æ¨™ç±¤æ¬„ä½ â€”â€”â€”
if __name__ == "__main__":
    # ä¾ä½ çš„å¯¦éš›è·¯å¾‘ä¿®æ”¹ï¼ˆWindows ç¯„ä¾‹ï¼šè«‹ç¢ºèªè·¯å¾‘å­˜åœ¨ï¼‰
    DATA_FILE = "C:\\Users\\User\\Desktop\\å·²å€åˆ†\\è¨“ç·´é›†\\1018_å…¨æ•¸æ“š_è¨“ç·´.xlsx"
    TARGET    = "label"   # å¦‚æœä½ çš„æ¨™ç±¤æ¬„ä½ä¸æ˜¯ labelï¼Œæ”¹é€™è£¡
    train_and_save_confusion(DATA_FILE, TARGET)
